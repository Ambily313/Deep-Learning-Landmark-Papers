#  Deep Learning Landmark Papers

Welcome to this curated collection of foundational and breakthrough research papers in deep learning. This repository aims to help students, researchers, and professionals explore the key ideas that shaped modern AI.

> ğŸ“š "If I have seen further, it is by standing on the shoulders of giants." â€” Isaac Newton

---


##  Artificial Neural Networks (ANN)

- **Learning Representations by Backpropagating Errors**  
  *Rumelhart, Hinton, and Williams (1986)*  
  [ğŸ“„ Paper](https://www.nature.com/articles/323533a0)  
  Introduced backpropagation for training neural networks.

---

##  Convolutional Neural Networks (CNN)

- **ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)**  
  *Krizhevsky, Sutskever, and Hinton (2012)*  
  [ğŸ“„ Paper](https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)  
  First deep CNN to win ImageNet; triggered the modern deep learning era.

- **Very Deep Convolutional Networks for Large-Scale Image Recognition (VGGNet)**  
  *Simonyan and Zisserman (2014)*  
  [ğŸ“„ Paper](https://arxiv.org/abs/1409.1556)  
  Used 3Ã—3 filters in very deep architectures like VGG16 and VGG19.

- **Going Deeper with Convolutions (Inception/GoogLeNet)**  
  *Szegedy et al. (2014)*  
  [ğŸ“„ Paper](https://arxiv.org/abs/1409.4842)  
  Introduced inception modules using multiple filter sizes in parallel.

- **Deep Residual Learning for Image Recognition (ResNet)**  
  *He et al. (2015)*  
  [ğŸ“„ Paper](https://arxiv.org/abs/1512.03385)  
  Used skip connections to build ultra-deep networks (e.g., ResNet50).

### ğŸ§­ Visual Timeline of CNN Architectures  
- [ğŸ–¼ï¸ View Timeline](https://towardsdatascience.com/a-comprehensive-overview-of-modern-cnns-architecture-40c9b34d8e44)

---

## ğŸ” Recurrent Neural Networks (RNN)

- **Long Short-Term Memory (LSTM)**  
  *Hochreiter and Schmidhuber (1997)*  
  [ğŸ“„ Paper](https://www.bioinf.jku.at/publications/older/2604.pdf)  
  Solved vanishing gradient issues in RNNs using memory gates.

---

##  Transformers & Attention Mechanisms

- **Attention is All You Need**  
  *Vaswani et al. (2017)*  
  [ğŸ“„ Paper](https://arxiv.org/abs/1706.03762)  
  Introduced transformers, replacing recurrence with attention.

---

##  Regularization Techniques

- **Dropout: A Simple Way to Prevent Neural Networks from Overfitting**  
  *Srivastava et al. (2014)*  
  [ğŸ“„ Paper](https://jmlr.org/papers/v15/srivastava14a.html)  
  Randomly drops units during training to reduce overfitting.

---

## ğŸš€ Optimization Algorithms

- **Adam: A Method for Stochastic Optimization**  
  *Kingma and Ba (2015)*  
  [ğŸ“„ Paper](https://arxiv.org/abs/1412.6980)  
  Combines momentum and RMSProp; widely used optimizer.

---

##  Generative Models

- **Generative Adversarial Nets (GANs)**  
  *Goodfellow et al. (2014)*  
  [ğŸ“„ Paper](https://papers.nips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)  
  Introduced adversarial training with generator vs discriminator.

- **Auto-Encoding Variational Bayes (VAE)**  
  *Kingma and Welling (2013)*  
  [ğŸ“„ Paper](https://arxiv.org/abs/1312.6114)  
  Combines deep learning with probabilistic graphical models.

---

##  Self-Supervised & Contrastive Learning

- **SimCLR: A Simple Framework for Contrastive Learning of Visual Representations**  
  *Chen et al. (2020)*  
  [ğŸ“„ Paper](https://arxiv.org/abs/2002.05709)  
  Learns representations without labels using contrastive loss.

- **Bootstrap Your Own Latent (BYOL)**  
  *Grill et al. (2020)*  
  [ğŸ“„ Paper](https://arxiv.org/abs/2006.07733)  
  Self-supervised learning without contrastive pairs.

---

##  Other Foundational Works

- **Deep Learning**  
  *LeCun, Bengio, and Hinton (2015)*  
  [ğŸ“„ Paper](https://www.nature.com/articles/nature14539)  
  A high-level review of deep learning by pioneers in the field.

- **Playing Atari with Deep Reinforcement Learning (DQN)**  
  *Mnih et al. (2013)*  
  [ğŸ“„ Paper](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)  
  Combined CNNs with Q-learning to play Atari games.

---

## ğŸ™Œ Contributing

Suggestions, additions, or corrections are welcome!  
Please open an issue or submit a pull request.

---

## â­ï¸ Acknowledgements

Thanks to the research community for these groundbreaking contributions. This list will continue to grow and evolve.

---



